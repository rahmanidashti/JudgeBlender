{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import krippendorff\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmjudge_test_path = \"#path to the test set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading LLMJudge test data\n",
    "llmjudge_test = pd.read_csv(llmjudge_test_path, sep=\" \", header=None, names=['qid', 'Q0', 'docid', 'score'])\n",
    "llmjudge_test['score'] = [0 if x < 0 else 3 if x > 3 else x for x in llmjudge_test['score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = llmjudge_test\n",
    "all_results = all_results.drop('Q0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "MODELS = ['blender1', 'blender2', 'blender3']\n",
    "\n",
    "for model in MODELS:\n",
    "    model_df = pd.read_csv(f\"judgments/{model}.txt\", sep=\" \", header=None, names=['qid', 'Q0', 'docid', 'score'])\n",
    "    model_df['score'] = [0 if x < 0 else 3 if x > 3 else x for x in model_df['score']]\n",
    "    model_df = model_df.drop('Q0', axis=1)\n",
    "    all_results = pd.merge(all_results, model_df, on=['qid', 'docid'], suffixes=('_main', '_X'))\n",
    "    all_results = all_results.rename({'score_main': 'score', 'score_X': f'{model}'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_results = all_results.drop('qid', axis=1)\n",
    "all_models_results = all_models_results.drop('docid', axis=1)\n",
    "all_models_results = all_models_results.drop('score', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(t, no_major=\"random\"):\n",
    "    # Set the seed for reproducibility\n",
    "    random.seed(42)\n",
    "    \"\"\"\n",
    "    Returns the majority element in a tuple of three integers.\n",
    "    If there is no majority element, returns None.\n",
    "    \"\"\"\n",
    "    if t[0] == t[1] or t[0] == t[2]:\n",
    "        return t[0]\n",
    "    elif t[1] == t[2]:\n",
    "        return t[1]\n",
    "    else:\n",
    "        if no_major == \"random\":\n",
    "            return random.choice(t)\n",
    "        elif no_major == \"max\":\n",
    "            return max(t)\n",
    "        elif no_major == \"min\":\n",
    "            return min(t)\n",
    "        elif no_major == \"avg\":\n",
    "            return round(np.mean(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_voting(t):\n",
    "    score_avg = np.mean(t)\n",
    "    score_avg = round(score_avg)\n",
    "    return score_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cohen_kappa(test_scores, submission_scores):\n",
    "    return round(cohen_kappa_score(test_scores, submission_scores), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_krippendorff_alpha(test_scores, submission_scores):\n",
    "    krippendorff_alpha = round(krippendorff.alpha(reliability_data=[test_scores, submission_scores], value_domain=[0,1,2,3], level_of_measurement='ordinal'), 4)\n",
    "    return krippendorff_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_blender_scores(filename, scores):\n",
    "    blender_scores_fl = open(f\"submissions/blenders/selected/{filename}.txt\", 'w')\n",
    "    for (qid, docid), score in zip(zip(all_results['qid'], all_results['docid']), scores):\n",
    "        blender_scores_fl.write(f\"{qid} 0 {docid} {score}\\n\")\n",
    "    blender_scores_fl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "blender_type = \"llm\" # prompt\n",
    "# MV: Majority Voting + (Avg, Rnd, Min, Max)\n",
    "# Avg\n",
    "aggregator_type = \"Avg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all combinations of 3 columns\n",
    "for cols in tqdm(itertools.combinations(all_models_results.columns, 3)):\n",
    "    blender_scores = list()\n",
    "    # compute each column's corr with test\n",
    "    kappa_1 = get_cohen_kappa(all_results['score'], all_models_results[cols[0]])\n",
    "    kappa_2 = get_cohen_kappa(all_results['score'], all_models_results[cols[1]])\n",
    "    kappa_3 = get_cohen_kappa(all_results['score'], all_models_results[cols[2]])\n",
    "\n",
    "    for scores in zip(all_models_results[cols[0]], all_models_results[cols[1]], all_models_results[cols[2]]):\n",
    "        score = avg_voting(scores) # majority_vote(scores, no_major=\"random\") # majority_vote(scores, no_major=\"avg\") # avg_voting(scores)\n",
    "        blender_scores.append(score)\n",
    "\n",
    "    blender_kappa = get_cohen_kappa(all_results['score'], blender_scores)\n",
    "    blender_alpha = get_krippendorff_alpha(all_results['score'], blender_scores)\n",
    "    write_blender_scores(f\"{blender_type}_{aggregator_type}\", blender_scores)\n",
    "    print(blender_kappa, blender_alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
